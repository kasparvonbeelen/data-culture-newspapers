{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyO2NbG/F0T5elTKcsnzRVcR",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kasparvonbeelen/data-culture-newspapers/blob/llms/1_Introduction_and_PLMs.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Overview\n",
        "\n",
        "This session reflects on the application of language models to research in digital humanities. It will be interactive, so you can play around with the examples and code snippets yourself.\n",
        "\n",
        "- This is a gentle introduction, open to everyone!\n",
        "- Also, still under construction.\n",
        "\n",
        "\n",
        "## What's on the menu?\n",
        "\n",
        "- Intro: What are language models, actually\n",
        "\n",
        "- Language Models as Models of Language: Experiments with historical and queer GPT-2 and BERT models\n",
        "- Using Instruction-tuned (or \"chat\" models) for distant reading\n",
        " - \"Don't count, summarize?\"\n",
        " - A simple RAG pipeline to investigate accidents in the news\n",
        " - Using LLMs for annotating and structuring newspaper data\n"
      ],
      "metadata": {
        "id": "2Q0-0NEQP9T5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Language Models as Models of Language"
      ],
      "metadata": {
        "id": "4wtltKK7Z_OB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## What are language models?\n",
        "\n",
        "LMs tell us what word is likely to follow a given sequence. More technically:\n",
        "\n",
        "> “[Language models] assign a probability* to each possible next word. (Jurafsky & Martin)”\n",
        "- Given a vocabulary *V*, which *w* (word) in *V* is likely to follow a sequence *s*?\n",
        "\n",
        "Given the sentence **“Predicting the future is hard, but not …”**\n",
        "\n",
        "- P(“impossible” | sentence) is greater than P(“aardvark” | sentence)\n",
        "\n",
        "\n",
        "> Read P(“impossible” | sentence) as the probability of observing the token “impossible” given the sequence “Predicting the future is hard, but not ...\n",
        "\n",
        "\n",
        "> Probabilities are values between 0 and 1 that sum up to 1.\n",
        "\n",
        "## How are Language models created?\n",
        "\n",
        "- **Pre-training** using a language modelling task: iterate over a large collection of text and improve the model's performance on the next token prediction task. In the process the model learns a lot about language, society and the \"real\" world.\n",
        "- **Instruction tuning** improves the model so that it follows instructions correctly.\n",
        "\n",
        "E.g. [Llama-3-8B](https://huggingface.co/meta-llama/Meta-Llama-3-8B) vs. [Llama-3-8B-Instruct](https://huggingface.co/meta-llama/Meta-Llama-3-8B-Instruct)"
      ],
      "metadata": {
        "id": "XE5VaviwSGxk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Language Models One Token at a Time\n",
        "## Next word prediction with GPT-2\n",
        "\n",
        "Next word prediction is the principal building block of generative AI applications. We'll also encounter it when playing with larger language models.\n",
        "\n",
        "In the following example, we generate just one token. We will show how a language model produces a probability distribution over the vocabulary from which it can sample the next token."
      ],
      "metadata": {
        "id": "qvUHPnaYSMGo"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bGMUQaYDP3Ta"
      },
      "outputs": [],
      "source": [
        "%%bash\n",
        "pip install -q transformers accelerate datasets shap"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer, GPT2Model, pipeline\n",
        "import numpy as np\n",
        "import shap\n",
        "from torch.nn import Softmax\n",
        "import pandas as pd\n",
        "softmax = Softmax(dim=0) # initialize softmax function"
      ],
      "metadata": {
        "id": "GM4khPYJSaPe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#prompt = 'Hello my name is John' # define a prompt\n",
        "prompt = 'Predicting the future is hard, but not'"
      ],
      "metadata": {
        "id": "35qYomYVSe9r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# tokenizer will split a text in units the LM is built on\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
        "# add the EOS token as PAD token to avoid warnings\n",
        "model = GPT2LMHeadModel.from_pretrained(\"gpt2\", pad_token_id=tokenizer.eos_token_id)\n",
        "# load the gpt-2 model\n",
        "gpt2 = GPT2Model.from_pretrained(\"gpt2\", pad_token_id=tokenizer.eos_token_id)\n",
        "# get logits from model\n",
        "predictions = model(**tokenizer(prompt, return_tensors='pt'))\n",
        "# the predictions as logits\n",
        "# predictions.logits.shape\n",
        "# get words with highest probability\n",
        "tokenizer.decode(np.argmax(predictions.logits[0,-1,:].detach().numpy()))\n",
        "# order predictions\n",
        "series = pd.Series(softmax(predictions.logits[0,-1,:]).detach()).sort_values(ascending=False)\n",
        "# change token_ids to the actual tokens\n",
        "index = [tokenizer.decode(x) for x in series.index]\n",
        "# set tokens as index\n",
        "series.index = index\n",
        "# plot results\n",
        "series[:100].plot(kind='bar',figsize=(20,5))"
      ],
      "metadata": {
        "id": "2KK5n8oJSmGU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# From predicting the next token to generating complete documents\n",
        "\n",
        "To generate longer text we repeat the next token prediction multiple times until we encounter a stop symbol (or the limit of tokens to generate).\n",
        "\n",
        "Text generation involves the following steps:\n",
        "\n",
        "- Create a probability distribution over the next token\n",
        "- Sample a token based on this distribution*\n",
        "- Add the sampled token to the input sequence, and repeat...\n",
        "\n",
        "\n",
        "* we have a few tricks up our sleeves here: we can manipulate the sampling procedure using specific hyperparameters, such as `temperature`, `top_k`, `top_p`\n",
        "\n",
        "#### An example of temperature\n",
        "Increasing the temperature can make predictions more creative (or random if you [like](https://medium.com/mlearning-ai/softmax-temperature-5492e4007f71#:~:text=Temperature%20is%20a%20hyperparameter%20of%20LSTMs%20(and%20neural%20networks%20generally,utilize%20the%20Softmax%20decision%20layer.)))\n",
        "\n",
        "![temperature](https://miro.medium.com/v2/resize:fit:1400/format:webp/0*7xj72SjtNHvCMQlV.jpeg)\n",
        "\n",
        "Image taken for this [blog post](https://medium.com/mlearning-ai/softmax-temperature-5492e4007f71#:~:text=Temperature%20is%20a%20hyperparameter%20of%20LSTMs%20(and%20neural%20networks%20generally,utilize%20the%20Softmax%20decision%20layer.) on temperature in Softmax."
      ],
      "metadata": {
        "id": "8E947ZGmS0uF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The generation steps outlined above are neatly integrated in the Hugging Face 'text-generation' `pipeline`.\n",
        "\n",
        "First we instantiate the `pipeline` using the GPT2 model, the predecessor the now famous GPT3."
      ],
      "metadata": {
        "id": "tD612kvDojAw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer_gpt2 = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
        "generator_gpt2 = pipeline('text-generation', # define the task\n",
        "                     model = 'gpt2', # define the model\n",
        "                     pad_token_id=tokenizer.eos_token_id)\n"
      ],
      "metadata": {
        "id": "8tvVQJ6-SmIo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Then we have to define a prompt (or input text) for which we want to predict the next token."
      ],
      "metadata": {
        "id": "6Zf1XpyQo731"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = 'Predicting the future is hard, but not' # 'Hello, my name is'"
      ],
      "metadata": {
        "id": "3QZhzrDwfTvJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lastly, we can generate a few texts to follow the given prompt. We limit the length of these completions to 30 tokens. We use a low temperature so things won't get too weird!"
      ],
      "metadata": {
        "id": "UuKMdvPfpK04"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "completions = generator_gpt2(prompt,\n",
        "            temperature=0.1, # increase temperature for more create generations\n",
        "            max_length = 30,  # max length of each generated text\n",
        "            num_return_sequences=3 # how many texts to generate\n",
        "         )\n",
        "completions"
      ],
      "metadata": {
        "id": "bQzgKX3kX0Ke"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Models ❤️ Data\n",
        "\n",
        "\n",
        "\n",
        "- **Models \"mimic\" or \"parrot\" the data they are trained on.** Some scholars argue they should be referred to as **\"corpus models\"** instead of \"language models\". In this context, language models are understood to be **\"compression of data\"**, as their weights encode important patterns in the data. In this sense, language models can be used to investigate complex patterns and regularities in language, which I think is of interest to scholars in the digital humanities.\n",
        "- **Different training data results in different model (outputs).** We can compare models trained in different data to study how language changes over time or by context. In these scenarios, the language models themselves are valuable objects of study, especially when combined with tools/methods that allow us to **interpret** the behaviour of these models.\n",
        "- Below we have a closer look at some examples. First, we compare the vanilla GPT2 models to one fine-tuned newspaper article on Brexit (based on this wonderful programming historian [tutorial](https://programminghistorian.org/en/lessons/interrogating-national-narrative-gpt)) and a selection of [Queer 80s literature](https://www.london.ac.uk/about/services/senate-house-library/exhibitions/seized-books).\n"
      ],
      "metadata": {
        "id": "IDWQ_ApsTMnj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We need to import some required tools and libraries."
      ],
      "metadata": {
        "id": "IJPb7ZsGr_Nh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
        "import shap # we use this tool to visualize model prediction"
      ],
      "metadata": {
        "id": "qJaxp4TpW57n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We load the different language models: `gpt2`, `QueerGPT2` and `gpt-brexit` (both still under construction)."
      ],
      "metadata": {
        "id": "t5NER4ZzsEIx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "explainer_dict = {}\n",
        "\n",
        "for checkpoint in  ['gpt2','Kaspar/QueerGPT2','Kaspar/gpt-brexit']:\n",
        "  tokenizer = AutoTokenizer.from_pretrained('gpt2', use_fast=True)\n",
        "  model = AutoModelForCausalLM.from_pretrained(checkpoint)\n",
        "  wrapped_model = shap.models.TopKLM(model, tokenizer, k=25)\n",
        "  masker = shap.maskers.Text(tokenizer, mask_token=\"...\", collapse_mask_token=True)\n",
        "  explainer = shap.Explainer(wrapped_model, masker)\n",
        "  explainer_dict[checkpoint] = explainer"
      ],
      "metadata": {
        "id": "v0mlOPBXW04W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Interpreting model predictions with shap"
      ],
      "metadata": {
        "id": "8wASo_lgu9-H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The `shap` library nicely visualises which parts of the input segment influence the prediction of the next token.\n",
        "\n",
        "### Exercise:\n",
        "\n",
        "Change the prompt and look at how the predictions differ in relation to the input text."
      ],
      "metadata": {
        "id": "TWo_CzLzsbDW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"First I went to walk the dog, then I had breakfast, now I will go to the\" # change the prompt\n",
        "expl = 'gpt2'\n",
        "shap_values = explainer_dict[expl]([prompt])\n",
        "shap.plots.text(shap_values)"
      ],
      "metadata": {
        "id": "YxaGs8wEWuVe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Importantly, if we fine-tune the model on different data, it gains novel knowledge and will change its behaviour and predictions."
      ],
      "metadata": {
        "id": "1Q_xHYqKuANu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"I want the United Kingdom to stay in the European Union. I will vote for\" # change the prompt\n",
        "expl = 'gpt2' # gpt2 | Kaspar/gpt-brexit\n",
        "shap_values = explainer_dict[expl]([prompt])\n",
        "shap.plots.text(shap_values)"
      ],
      "metadata": {
        "id": "BPB6CNOaYpWU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# tokens predicted by QueerGPT\n",
        "prompt = \"When I grow up, I want to become a\" # change the prompt\n",
        "expl = 'Kaspar/QueerGPT2' # 'Kaspar/QueerGPT2' | gpt2\n",
        "shap_values = explainer_dict[expl]([prompt])\n",
        "shap.plots.text(shap_values)"
      ],
      "metadata": {
        "id": "o43f7MgxdqeS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lastly, we can generate longer outputs and assess how these related to input tokens."
      ],
      "metadata": {
        "id": "Dnq3N3xwu3A4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"I want the United Kingdom to stay in the European Union. Therefore, I will vote for\" # change the prompt\n",
        "checkpoint = 'Kaspar/gpt-brexit' #  choose gpt2 or Kaspar/QueerGPT2\n",
        "tokenizer = AutoTokenizer.from_pretrained('gpt2', use_fast=True)\n",
        "model = AutoModelForCausalLM.from_pretrained(checkpoint)\n",
        "# set model decoder to true\n",
        "model.config.is_decoder = True\n",
        "# set text-generation params under task_specific_params\n",
        "model.config.task_specific_params[\"text-generation\"] = {\n",
        "    \"do_sample\": True,\n",
        "    \"max_length\": 200,\n",
        "    \"temperature\": 0.1,\n",
        "    \"top_k\": 50,\n",
        "    #\"no_repeat_ngram_size\": 2,\n",
        "}\n",
        "explainer = shap.Explainer(model, tokenizer)\n",
        "shap_values = explainer([prompt])"
      ],
      "metadata": {
        "id": "4HaXc2Jmdl8E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "shap.plots.text(shap_values)"
      ],
      "metadata": {
        "id": "3fiKNdXQebNR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Historical Language Models\n",
        "\n",
        "Comparing model predictions allows us to study linguistic and societal changes. In the Living with Machines project, we investigated the concept of atypical animacy, focussed on the portrayal of machines as being 'alive'.\n",
        "\n",
        "You can read the technical paper [here](https://arxiv.org/abs/2005.11140) and the historical article [here](https://muse.jhu.edu/pub/1/article/903976/summary).\n",
        "\n",
        "In these papers, we used a slightly different technique and type of model, namely autoencoding/masked language models (as opposed to the GPT-series which resort under the autoregressive/causal category).\n",
        "\n",
        "We masked the word 'machine' and investigated what the model predicts, focussing on examples where it predicts humans or animals instead of mechanical objects.\n",
        "\n",
        "Moreover, we used historical language models, to study how these predictions change by period.\n",
        "\n",
        "We fine-tuned BERT models on 19th-century book collections as described in [our paper](https://openhumanitiesdata.metajnl.com/articles/10.5334/johd.48)."
      ],
      "metadata": {
        "id": "SXeI20OpUg5G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "sentence = \"Our sewing [MASK] stood near the wall where grated windows admitted sunshine, and their hymn to Labour was the only sound that broke the brooding silence.\""
      ],
      "metadata": {
        "id": "e9fYSe13yNzC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "masker = pipeline(\"fill-mask\", model='bert-base-uncased')\n",
        "masker(sentence)"
      ],
      "metadata": {
        "id": "QroaZX8PUj48"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "victorian_masker = pipeline(\"fill-mask\", model='Livingwithmachines/bert_1760_1850')\n",
        "victorian_masker(sentence)"
      ],
      "metadata": {
        "id": "DWCi81MXWPsz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exercise\n",
        "\n",
        "Can you think of another example where we might observe interesting historical differences? Change the sentence variable with another \"[MASK]\" token."
      ],
      "metadata": {
        "id": "0XowxXzHyFtk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Fin."
      ],
      "metadata": {
        "id": "kYE04Stmx5ji"
      }
    }
  ]
}