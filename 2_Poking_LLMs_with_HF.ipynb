{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kasparvonbeelen/data-culture-newspapers/blob/llms/2_Poking_LLMs_with_HF.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S-2ldxb0iUxG"
      },
      "source": [
        "# Using open-source LLMs for analysing humanities data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E_e5BL9Ti3JG"
      },
      "source": [
        "In this notebook, we explore applications of generative AI for processing and analysing historical newspapers.\n",
        "\n",
        "Instead of investigating how the model works, we focus on what we can do with the outputs.\n",
        "\n",
        "Major hurdles to working with LLMs are cost and/or infrastructure. Opposed to GPT-2 or BERT, running LLMs can be difficult, and using commercial APIs can be expensive.\n",
        "\n",
        "## Why Open-source?\n",
        "\n",
        "- **Privacy:**: You might not want to share your data (and ideas) with companies such as OpenAI;\n",
        "- **Cost:** Making abstraction of the caveat above, using open-source models might reduce costs if you want to apply for example a prompt to 10k newspaper articles;\n",
        "- **Transparency:** Be mindful that there are different gradations of openness and transparency. Even when you can access the model weights, you might remain in the dark about training data and other factors);\n",
        "- **Flexibility:** Even though some providers allow you to train or fine-tune closed models on your data (ties in with privacy), open-source models still give you more freedom and wiggle room to build new models and applications.\n",
        "\n",
        "## Goals of this Session\n",
        "\n",
        "This notebook covers a few practical and theoretical aspects of working with LLMs in the context of humanities research. The goal is to start a discussion on:\n",
        " - Where to find and how to deploy open-source LLMs?\n",
        " - What tasks would make sense? Which models work well for a selected task?\n",
        " - How to evaluate outcomes and performance? How large should the language model be?\n",
        "\n",
        "We want to keep things simple!\n",
        "\n",
        "We will be playing with Llama-3 and get a feeling of how this changes the way we process and interrogate data.\n",
        "\n",
        "\n",
        "## Technical note\n",
        "\n",
        "We will be relying on the Hugging Face `InferenceClient` for accessing LLMs. These are freely accessible, but rate limits apply! If you would want to deploy a 'local' version (we're still on Colab, but the code should also work on your computer), uncomment the code below (where indicated) and make sure you are using a [GPU](https://cloud.google.com/gpu). To select a GPU on Colab Go to **`Runtime`** and select **`Change runtime type`**, then select `T4 GPU` (or any other GPU available).\n",
        "\n",
        "\n",
        "\n",
        "This notebook is inspired by: https://huggingface.co/learn/cookbook/structured_generation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "id": "qxa9ES0zX9ic"
      },
      "outputs": [],
      "source": [
        "# install the transformer and other libraries\n",
        "!pip install -q -U \"transformers==4.40.0\" pydantic accelerate outlines datasets bitsandbytes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CXUryNZSXUd8"
      },
      "source": [
        "## The Hugging Face Hub\n",
        "\n",
        "In the examples below, we will experiment with `Llama-3-8B-Instruct`, a recent series of open-source LLMs created by Meta. To use Llama3 you need to:\n",
        "\n",
        "- Make an account on Hugging Face https://huggingface.co/\n",
        "- Go to the Llama-3-8B and sign the terms of use you should get a reply swiftly https://huggingface.co/meta-llama/Meta-Llama-3-8B\n",
        "- Create a user access token with at least read access: https://huggingface.co/docs/hub/en/security-tokens\n",
        "- Run the code cell below to log into the Hugging Face hub. Copy-paste the access token.\n",
        "- Reply `n` to the question 'Add token as git credential? (Y/n)'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {
        "id": "KIuiPmuRV9aU"
      },
      "outputs": [],
      "source": [
        "!huggingface-cli login"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gpNX4yD1ch09"
      },
      "source": [
        "## Preparing model and data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RtfaU-z2cLRC"
      },
      "source": [
        "### Import libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "IivyFPPNQ0t-"
      },
      "outputs": [],
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore') # disable warnings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "27FpdOCLasrJ"
      },
      "outputs": [],
      "source": [
        "import transformers\n",
        "from huggingface_hub import InferenceClient\n",
        "from datasets import Dataset\n",
        "from tqdm import tqdm\n",
        "import pandas as pd\n",
        "import torch\n",
        "import pandas as pd\n",
        "import json\n",
        "pd.set_option(\"display.max_colwidth\", 100)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xtl7poXAcMdJ"
      },
      "source": [
        "### Load model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "6RiZovNJXdlf"
      },
      "outputs": [],
      "source": [
        "# choose a LLMs model\n",
        "repo_id = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
        "# instantiate the inference client\n",
        "llm_client = InferenceClient(model=repo_id, timeout=120)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "Uc9ROzqBjxan"
      },
      "outputs": [],
      "source": [
        "# # use this cell if you can access an A100 or L4 GPU\n",
        "# # define the model, we use the instruct variant\n",
        "# checkpoint = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
        "# device = 'cuda' # make sure you use a GPU\n",
        "\n",
        "# # instantiate a text generation pipeline\n",
        "# pipeline = transformers.pipeline(\n",
        "#     \"text-generation\",\n",
        "#     model=checkpoint,\n",
        "#     model_kwargs={\"torch_dtype\": torch.bfloat16},\n",
        "#     device=\"cuda\",\n",
        "# )\n",
        "\n",
        "# # some fluff to improve the generation\n",
        "# terminators = [\n",
        "#     pipeline.tokenizer.eos_token_id,\n",
        "#     pipeline.tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n",
        "# ]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# # use this cell if you can only access a T4 GPU\n",
        "# from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, pipeline\n",
        "# # define the model, we use the instruct variant\n",
        "# checkpoint = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
        "# device = 'cuda' # make sure you use a GPU if available\n",
        "\n",
        "# bnb_confic = BitsAndBytesConfig(\n",
        "#     load_in_4bit=True,\n",
        "#     bnb_4bit_use_double_quant=True,\n",
        "#     bnb_4bit_quant_type=\"nf4\",\n",
        "#     bnb_4bit_compute_dtype=torch.bfloat16\n",
        "# )\n",
        "\n",
        "# tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
        "# #tokenizer.pad_token = tokenizer.eos_token\n",
        "# model = AutoModelForCausalLM.from_pretrained(checkpoint,\n",
        "#                                              quantization_config=bnb_confic,\n",
        "#                                              device_map='auto')\n",
        "\n",
        "# pipeline = transformers.pipeline(\n",
        "#     \"text-generation\",\n",
        "#     model=model,\n",
        "#     tokenizer= tokenizer,\n",
        "# )\n",
        "\n",
        "\n",
        "# # some fluff to improve the generation\n",
        "# terminators = [\n",
        "#     pipeline.tokenizer.eos_token_id,\n",
        "#     pipeline.tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n",
        "# ]"
      ],
      "metadata": {
        "id": "KN-l5VRih9Zm",
        "outputId": "fe9b29aa-1c73-4a24-bfbd-a54b05220746",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0,
          "referenced_widgets": [
            "9a2f073bffe24c87abb9d81547b43311",
            "b2fb19d6b55c42a893b698ce22c02544",
            "ff43f1349df542bc9dbe21a98d32a809",
            "2e5dbb5d6c5d45afba265047753b9e20",
            "0e050d898f1643ca8a5c074a554286b4",
            "56faa1ef2ea6445a8fdf1614e75e1c08",
            "1451f4284e85462390c14e6057014c95",
            "6301aad1e0c046cba777e8eba409d55a",
            "28b8de963d02438ca4a9ce3ca7ae7f04",
            "03284201464b44d7b5d200035c3404c2",
            "4010561ef6e94f0caf25aa8166a49b06"
          ]
        }
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9a2f073bffe24c87abb9d81547b43311"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lk732HeUcOvO"
      },
      "source": [
        "### Download data\n",
        "\n",
        "We will be experimenting with a small set of 10k British newspaper articles provided by the [\"Heritage Made Digital\"](https://blogs.bl.uk/thenewsroom/2019/01/heritage-made-digital-the-newspapers.html) project. Data was kindly prepared and provided by my colleague [Nilo Pedrazzini](https://www.linkedin.com/in/nilopedrazzini)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {
        "id": "bj-797e0Z9zr"
      },
      "outputs": [],
      "source": [
        "# download a sample of 10.000 newspaper articles\n",
        "!wget -q --show-progress https://github.com/kasparvonbeelen/lancaster-newspaper-workshop/raw/wc/data/sample_lwm_hmd_mt90_10000.csv.zip\n",
        "# unzip the downloaded sample\n",
        "!unzip -o sample_lwm_hmd_mt90_10000.csv.zip\n",
        "!rm -r __MACOSX"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {
        "id": "CP6EV9WhZXUR"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv('sample_lwm_hmd_mt90_10000.csv')\n",
        "df.head(3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {
        "id": "tE6fnaaRZgzU"
      },
      "outputs": [],
      "source": [
        "df.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JYvK2zSTcUg4"
      },
      "source": [
        "### Process data\n",
        "\n",
        "To facilitate the analysis we divide the newspaper articles into smaller chunks of 250 words (with a 50-word overlap)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "fE6QOcPqaf1I"
      },
      "outputs": [],
      "source": [
        "def get_chunks(text: str, size: int=250,step: int=50) -> list:\n",
        "  \"\"\"divide a text into chunks of similar size\n",
        "  Arguments:\n",
        "    text (str): input text\n",
        "    size (int): number of tokens in each chunk\n",
        "    step (int): step size\n",
        "  Returns a list of strings\n",
        "  \"\"\"\n",
        "  words = text.split()\n",
        "  return [' '.join(words[i:i+size]) for i in range(0,len(words),step)]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FeSqqV6Wa2Ra"
      },
      "source": [
        "We save the chunks in a new list."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "Px_o2GN_Zn8K"
      },
      "outputs": [],
      "source": [
        "# apply chunking to text\n",
        "df['chunks'] = df.text.apply(get_chunks)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {
        "id": "NI7zi3fqZ-3o"
      },
      "outputs": [],
      "source": [
        "len(df.text[0]),len(df['chunks'][0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5kVFcYiK2j2Z"
      },
      "source": [
        "Next, we reorder the dataframe: each chunk of 250 will be a new row (this increases the number of rows quite a bit, as you may observe)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {
        "id": "vYGjL1nPZ4ZV"
      },
      "outputs": [],
      "source": [
        "# reorder the dataframe\n",
        "# with one chunk in each row\n",
        "# instead of the whole text\n",
        "df_chunks = df.explode('chunks')\n",
        "df_chunks.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QY1ucJhFnguw"
      },
      "source": [
        "## Prompting\n",
        "\n",
        "LLM generate text from an input, usually referred to as a 'prompt', a piece of text we like the model to use as a starting point for predicting novel tokens.\n",
        "\n",
        "When 'chatting' with an LLM we usually provide the model with (at least) two messages: a system and a user prompt or message.\n",
        "\n",
        "**System message**:\n",
        "\n",
        "- **Generic instructions on behaviour**: specify how the model should behave (e.g. be helpful, respectful, neutral) or the role it should play (e.g., a teacher, assistant, or advisor).\n",
        "- **Constraints**: Specific instructions on what the model should avoid or how it should generate responses.\n",
        "- **Context**: Background information or context that remains constant throughout the session to ensure consistency.\n",
        "\n",
        "**User message**:\n",
        "\n",
        "- **Query**: specifies input from the user, such as a question, instruction, or request that the model needs to respond to.\n",
        "- **Dynamic**: changes with each interaction, reflecting the user's immediate needs, questions, or instructions.\n",
        "\n",
        "The Hugging Face chat prompt template allows messages as lists of dictionaries.\n",
        "\n",
        "```python\n",
        "messages [\n",
        " {\n",
        "    \"role\" : \"system\",\n",
        "    \"content\": \"<system prompt here>\"\n",
        " },\n",
        " {\n",
        "    \"role\" : \"user\",\n",
        "    \"content\": \"<user prompt here>\"\n",
        " }\n",
        "]\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GNYCb3IqoRZ7"
      },
      "source": [
        "Define a message by articulating a system and user prompt."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "AO5PEGlsUukK"
      },
      "outputs": [],
      "source": [
        "messages = [\n",
        "    {\n",
        "        \"role\": \"system\",\n",
        "        \"content\": \"\"\"\n",
        "          You are a helpful AI that will assist me with analysing and reading newspaper articles.\n",
        "          Read the newspaper article attentively and extract the required information.\n",
        "          Each newspaper article is enclosed with triple hashtags (i.e. ###).\n",
        "          Don't make things up! If the information is not in the article then reply 'I don't know'\n",
        "          \"\"\"\n",
        "              },\n",
        "\n",
        "    {\n",
        "        \"role\": \"user\",\n",
        "        \"content\": f\"\"\"Provide a short description of principal characters portrayed in the newspaper article?\n",
        "\n",
        "                  ###POOR T,i,ENIPAT A 1„k CT  The Poor Law Coirdnissioti(rs have issued a ei; cular,\n",
        "                  dated the 20th instant, stating that they have consulted the Attorney and\n",
        "                  Solicitor-General on the construction of the late Removal Act, and give as the\n",
        "                  result:— I. \" That the proviso to the Ist section of the 9 and 10 Vict., c. 66,\n",
        "                  which sets forth the exceptions to the principal enactments that are to be\n",
        "                  excluded in the computation of time, is net retrospective in its operation, so\n",
        "                  as to apply to cases where the five years\\' residence was complete before the statute.\n",
        "                  2. \" That an interval between the completion of the five years residence and the\n",
        "                  application for the warrant of removal filled up by one of the exceptions contained\n",
        "                  in the proviso will not p event the operation of the statute in restraining the\n",
        "                  removal of the pauper whu had resided for the specified time. 3. \" That orders\n",
        "                  of removal obtained previous to th• passing of the Act, but not then executed\n",
        "                  by the removal of the paupers,###\"\"\"\n",
        "              }\n",
        "  ]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {
        "id": "zh2iS37UBIOU"
      },
      "outputs": [],
      "source": [
        "messages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "GyBwFdkBZ6RG"
      },
      "outputs": [],
      "source": [
        "#help(llm_client.chat_completion)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "-MM2Wlv_Vw3y"
      },
      "outputs": [],
      "source": [
        "# # uncomment this code if you want to work locally, comment the other function\n",
        "# def get_completion(messages: list, temperature=.1, top_p=.1) -> str:\n",
        "#   \"\"\"get completion for given system and user prompt\n",
        "#     Arguments:\n",
        "#     messages (list): a list containin a system and user message as\n",
        "#       python dictionaries with keys 'role' and 'content'\n",
        "#     temperature (float): regulate creativity of the text generation\n",
        "#     top_p (float): cummulative probability included in the\n",
        "#       generation process\n",
        "#   \"\"\"\n",
        "#   prompt = pipeline.tokenizer.apply_chat_template(\n",
        "#         messages,\n",
        "#         tokenize=False,\n",
        "#         add_generation_prompt=True\n",
        "#       )\n",
        "\n",
        "#   outputs = pipeline(\n",
        "#     prompt,\n",
        "#     max_new_tokens=256,\n",
        "#     eos_token_id=terminators,\n",
        "#     do_sample=True,\n",
        "#     temperature=temperature,\n",
        "#     top_p=top_p,\n",
        "#       )\n",
        "#   return outputs[0][\"generated_text\"][len(prompt):]\n",
        "\n",
        "# uncomment this if you are using the llm_client\n",
        "def get_completion(messages: list, temperature=.1, top_p=.1):\n",
        "    \"\"\"get completion for given system and user prompt\n",
        "      Arguments:\n",
        "        messages (list): a list containin a system and user message as\n",
        "          python dictionaries with keys 'role' and 'content'\n",
        "        temperature (float): regulate creativity of the text generation\n",
        "        top_p (float): cummulative probability included in the\n",
        "          generation process\n",
        "    \"\"\"\n",
        "    outputs = llm_client.chat_completion(\n",
        "        messages=messages,\n",
        "        max_tokens=1024,\n",
        "        temperature=temperature,\n",
        "        top_p=top_p\n",
        "        )\n",
        "    return outputs.choices[0].message.content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {
        "id": "f6G6Cbt18lUX"
      },
      "outputs": [],
      "source": [
        "print(get_completion(messages))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SML4OsbfXIk8"
      },
      "source": [
        "## Exercise\n",
        "\n",
        "- Change the system message and ask the model to reply in medieval French.\n",
        "- Change the user message and ask the model to summarize the article and condense it to one sentence."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "0CfZ-Q96omxn"
      },
      "outputs": [],
      "source": [
        "# Enter code here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tq0pWVJXoo10"
      },
      "source": [
        "#### Solution"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {
        "id": "am1Ge38tXGP-"
      },
      "outputs": [],
      "source": [
        "messages = [\n",
        "    {\"role\": \"system\", \"content\": \"\"\"\n",
        "    You are a helpful AI that will assist me with analysing and reading newspaper articles.\n",
        "    Read the newspaper article attentively and extract the required information.\n",
        "    Each newspaper article is enclosed with triple hashtags (i.e. ###).\n",
        "    Don't make things up! If the information is not in the article then reply 'I don't know'\n",
        "          Answer in medieval French!\"\"\"\n",
        "          },\n",
        "    {\"role\": \"user\", \"content\": f\"\"\"Provide a short description of principal characters portrayed newspaper article?\n",
        "    ###{df.iloc[0].text}###\"\"\"}\n",
        "]\n",
        "\n",
        "print(get_completion(messages))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gNcYq_a-beX1"
      },
      "source": [
        "## Applying text generation to historical documents\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "USeWpL8Ar-UX"
      },
      "source": [
        "### Example 1: Summarize\n",
        "\n",
        "Let's imagine we'd wish to know what happened in January 1899 but won't have time to read all the newspaper issues. Luckily, LLMs excel at summarization!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fy5lpuy94eBE"
      },
      "source": [
        "We select all the articles for this January 1899 and save them in a new dataframe. For the purposes of this exercise, we just take a random sample of 20 chunks, otherwise it will take too long to run everything through the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "metadata": {
        "id": "Cw-N3XoUkRfu"
      },
      "outputs": [],
      "source": [
        "df_small = df_chunks[\n",
        "            (df_chunks.year==1899) & (df_chunks.month==1) # select articles from January 1899\n",
        "                  ].sample(10, random_state=1984).reset_index(drop=True) # we sample a few to keep things simple\n",
        "df_small.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CIm6yKDE41Lo"
      },
      "source": [
        "Run the cell below to load the `apply_completions` function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "FJiND9t_dkE0"
      },
      "outputs": [],
      "source": [
        "def apply_completions(item: pd.Series,\n",
        "                      system_message: str,\n",
        "                      user_message: str,\n",
        "                      text_column: str = 'chunks') -> str:\n",
        "  \"\"\"\n",
        "  Function that appl\n",
        "  Argument:\n",
        "    item (pd.Series): row from a pandas Dataframe\n",
        "    system_message (str): system prompt, specifies how the system\n",
        "      should behave in\n",
        "    user_message (str): user prompt, give instruction how to\n",
        "      process each historical. the documents itself will be append\n",
        "      from the 'text_column' argument\n",
        "    text_column (str): name of the text column\n",
        "  \"\"\"\n",
        "  messages = [\n",
        "    {\"role\": \"system\", \"content\": system_message},\n",
        "    {\"role\": \"user\", \"content\": user_message}\n",
        "      ]\n",
        "  messages[1]['content'] += f\"\\n\\n###{item[text_column]}###\"\n",
        "  return  get_completion(messages)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OwuTEn3245AK"
      },
      "source": [
        "We apply the prompt to the text chunks in our dataframe."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "metadata": {
        "id": "nUEjoux9Y3ZV"
      },
      "outputs": [],
      "source": [
        "tqdm.pandas() # use tqdm to view progress\n",
        "\n",
        "system_message = \"\"\"\n",
        "    You are a helpful AI that will assist me with analysing and reading newspaper articles.\n",
        "    Read the newspaper article attentively and extract the required information.\n",
        "    Each newspaper article is enclosed with triple hashtags (i.e. ###).\n",
        "    Don't make things up! If the information is not in the article then reply 'I don't know'\n",
        "    \"\"\"\n",
        "user_message = \"Summarize the article in one sentence.\"\n",
        "\n",
        "df_small['completion'] =  df_small.progress_apply(apply_completions,system_message=system_message, user_message=user_message, axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "metadata": {
        "id": "n54N-ySxjuN2"
      },
      "outputs": [],
      "source": [
        "#print the summaries\n",
        "df_small['completion'][0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JzAyo9DJKhuO"
      },
      "source": [
        "### Example 2: Analyse information about accidents in the news\n",
        "\n",
        "In this example we complicate matters a little bit more.\n",
        "\n",
        "First we retrieve a set of documents based on the date of publications and their content. Then we use an LLMs to ask specific questions about this document ('a baby RAG pipeline, in the sense that we first retrieve and then generate a response to our query').\n",
        "\n",
        "How did accidents in the news change over time? Who is blamed for the accident?\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "In the first step we simple use a regular expression to find reports about accidents."
      ],
      "metadata": {
        "id": "58y6DHQsaPQq"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 81,
      "metadata": {
        "id": "TDv5HAz1LpW4"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "pattern = re.compile(r'\\baccidents?\\b', re.I) # compile a regex\n",
        "pattern.findall('accidents accident AccIdent accidental') # test the regex on a few example"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "metadata": {
        "id": "64Q74D2MMISw"
      },
      "outputs": [],
      "source": [
        "tqdm.pandas()\n",
        "df_chunks['matches'] = df_chunks.chunks.progress_apply(lambda x: bool(pattern.findall(x)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KpUwL6O3O3cw"
      },
      "source": [
        "Then we retrieve a small sample of accident reports during the 1810s."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 83,
      "metadata": {
        "id": "3j0Vs5IxMiUO"
      },
      "outputs": [],
      "source": [
        "accident_1810s = df_chunks[\n",
        "                    (df_chunks.year.between(1810,1820)) & (df_chunks['matches'] == True)\n",
        "                      ].sample(n=10, random_state=1984)\n",
        "\n",
        "print(accident_1810s.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WGHaHfoAPGTH"
      },
      "source": [
        "You can use `.value_counts()` to compute the total number of articles mentioning 'accident' at least once."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 84,
      "metadata": {
        "id": "tBvyenk2W7QC"
      },
      "outputs": [],
      "source": [
        "(df_chunks['matches'] == True).value_counts()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "ATo5AvhmYryp"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 85,
      "metadata": {
        "id": "1FtRSk2oT5sc"
      },
      "outputs": [],
      "source": [
        "system_message = \"\"\"\n",
        "    You are a helpful AI that will assist me with analysing and reading newspaper articles.\n",
        "    Read the newspaper article attentively and extract the required information.\n",
        "    Each newspaper article is enclosed with triple hashtags (i.e. ###).\n",
        "    Don't make things up! If the information is not in the article then reply 'I don't know'\n",
        "    Focus on the answer and do not add any unnecessary texts.\"\"\"\n",
        "user_message = \"\"\"Does the article talk about an accident?\n",
        "If yes, who is blamed for causing the accident? Is the accident caused by human error or a fault of the machine?\n",
        "If not, answer 'No accident mentioned' \"\"\"\n",
        "\n",
        "accident_1810s['completion'] =  accident_1810s.progress_apply(apply_completions,system_message=system_message, user_message=user_message, axis=1)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "accident_1810s['completion']"
      ],
      "metadata": {
        "id": "ZyNq1BaKapNM"
      },
      "execution_count": 86,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 87,
      "metadata": {
        "id": "c46YdAkqU9_K"
      },
      "outputs": [],
      "source": [
        "accident_1810s[['chunks','completion']].iloc[8].values"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ov1ySBNokZmj"
      },
      "source": [
        "### Example 3: Structured Generation\n",
        "\n",
        "Newspapers contain a lot of biographical information, one could say biography appears as a microgenre in the press. For example, in accident reports we do get some background about the people involved, implicitly (gender) or explicitly (professions or age).\n",
        "\n",
        "Below we use a language model to extract such information from newspaper reports and return it in a predefined format that allows us to analyse newspapers as structured data.\n",
        "\n",
        "Put differently, we use LLMs to extract information similar to automatic annotation, and convert text to tabular format."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "wKWWeBVXoFaG"
      },
      "outputs": [],
      "source": [
        "df_small = df_chunks[\n",
        "                    (df_chunks['matches'] == True)\n",
        "                      ].sample(n=10, random_state=1984)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "wCE10sOIZSzt"
      },
      "outputs": [],
      "source": [
        "# df_small['chunks'].iloc[7]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cDSoGMluRrVa"
      },
      "source": [
        "We rewrite the system prompt and give it a few more instructions on how to respond to our queries."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "xAYqIGPOheVr"
      },
      "outputs": [],
      "source": [
        "system_message = \"\"\"You are an helpful AI that will assist me with analysing source documents in the form of historical newspaper articles.\n",
        "    Read the newspaper articles attentively and extract structured information formatted as a list of Python dictionaries.\n",
        "    Provide all relevant short source snippets from the documents on which you directly based your answer.\n",
        "    Keep the source snippet short to just a few words and not complete sentences.\n",
        "    The snippet MUST be extracted from the soutce, with spelling and wording identical to the source.\n",
        "    This list of JSON blobs should begin with a \"START\" tag and end with a \"END\" tag.\n",
        "    Each newspaper article will be enclosed with triple hash tags (i.e. ###).\n",
        "    Don't make thigs up! If you don't know the answer, simply return no value\"\"\"\n",
        "\n",
        "\n",
        "user_message = \"\"\"\n",
        "If the article describes a historical accident, extract biographical information about the individuals involved in the accidents.\n",
        "Return a list of Python dictionaries for each individual which records important personal attributes such gender, age and profession, and others that are relevant.\n",
        "Each attribute is a key in a dictionary.\n",
        "Record personal attribures as dictionaries as shown in the example below.\n",
        "Also add one key with \"outcome\" that records what happened to person (\"drowned\", \"survived\", \"injured\")\n",
        "Add a confidence score as a float between 0 and 1 for each snippet extracted.\n",
        "Under \"source_snippets\" collect text fragments that record what happened to person involved.\n",
        "\n",
        "START\n",
        "[\n",
        "  {\n",
        "  \"name\" : { \"value\": answer,\"source\": source_snippet, \"confidence\": your_confidence_score },\n",
        "  \"gender\" : { \"value\": answer,\"source\": source_snippet, \"confidence\": your_confidence_score },\n",
        "  \"profession\" :{ \"value\": answer,\"source\": source_snippet, \"confidence\": your_confidence_score },\n",
        "  ... other attributes ...,\n",
        "  \"outcome\" : { \"value\": answer,\"source\": source_snippet, \"confidence\": your_confidence_score },\n",
        "  \"summary\": { \"value\" :summary, \"confidence\" : your_confidence_score }\n",
        "  },\n",
        "...]\n",
        "END\n",
        "\"\"\"\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 89,
      "metadata": {
        "id": "3S9M6ojCd0d7"
      },
      "outputs": [],
      "source": [
        "messages = [\n",
        "    {\"role\": \"system\", \"content\": system_message},\n",
        "    {\"role\": \"user\", \"content\": user_message + f'\\n\\n###{df_small[\"chunks\"].iloc[4]}###'}\n",
        "      ]\n",
        "print(get_completion(messages))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 90,
      "metadata": {
        "id": "cDw5FzAta9Ku"
      },
      "outputs": [],
      "source": [
        "df_small['completion'] =  df_small.progress_apply(apply_completions,system_message=system_message, user_message=user_message, axis=1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 91,
      "metadata": {
        "id": "BJGmFs9Qq_Tu"
      },
      "outputs": [],
      "source": [
        "print(df_small['completion'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ocO1ody2SAWt"
      },
      "source": [
        "To convert the response to a Python data type, we use the `eval_completion` function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 92,
      "metadata": {
        "id": "NmyQ5u4NIIT_"
      },
      "outputs": [],
      "source": [
        "def eval_completion(completion: str) -> list:\n",
        "  \"\"\"Convert the completion as string to a Python list\n",
        "  Argument:\n",
        "      completion (str): structured generation by LLM\n",
        "  \"\"\"\n",
        "  try:\n",
        "    return eval(completion.split('START')[-1].strip().rstrip('END').strip())\n",
        "  except Exception as e:\n",
        "    print(e)\n",
        "    return []\n",
        "\n",
        "df_small['completion_eval'] = df_small['completion'].apply(eval_completion)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CBwc1JXhSgK1"
      },
      "source": [
        "Let's have a bit closer look at some examples."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 93,
      "metadata": {
        "id": "0boRbvyXIsdW"
      },
      "outputs": [],
      "source": [
        "df_small['completion_eval']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 94,
      "metadata": {
        "id": "tS6AD3QbpL0t"
      },
      "outputs": [],
      "source": [
        "df_small['completion_eval'].iloc[8]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WXbqMNySJ2ii"
      },
      "source": [
        "Lastly we can have a bit closer look at how the language model processes the text by highlighting the fragments on which it based its answers. This can help us with\n",
        "- creating automatic pre-annotation\n",
        "- figuring out how the pipeline could be improved\n",
        "- close-reading large amounts of text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "NZD1DtdgJqPG"
      },
      "outputs": [],
      "source": [
        "row = df_small.iloc[8]\n",
        "html_output = row['chunks']\n",
        "for p_dict in row['completion_eval']:\n",
        "  for attr, attr_dict in p_dict.items():\n",
        "    try:\n",
        "      if isinstance(attr_dict, dict):\n",
        "        if attr_dict.get('confidence',.0) > .5 and attr_dict.get(\"source\",None):\n",
        "          html_output = re.sub(str(attr_dict['source']),\n",
        "                   f'<span style=\"background-color: yellow;\">{attr_dict[\"source\"]}</span>', html_output)\n",
        "    except Exception as e:\n",
        "      print(e,attr_dict)\n",
        "      continue"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 95,
      "metadata": {
        "id": "41udlbVTLeLr"
      },
      "outputs": [],
      "source": [
        "from IPython.core.display import HTML\n",
        "HTML(html_output)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CWAog9xdpG_q"
      },
      "source": [
        "### Example 4: OCR correction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jmvJn6ZlOdC4"
      },
      "source": [
        "Lastly, let's use LLM to help us with a longstanding problem in digital humanities, improving OCR quality."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "G5N8e2qdpKBZ"
      },
      "outputs": [],
      "source": [
        "df_small_bad_ocr = df_chunks.sort_values('ocrquality', ascending=True)[:1000].sample(n=10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 96,
      "metadata": {
        "id": "4JXH_oypolYi"
      },
      "outputs": [],
      "source": [
        "system_message = \"You are an helpful AI and provide truthful correction of historical text.\"\n",
        "\n",
        "user_message = \"\"\"Transcribe the text and correct typos and errors in the text caused by bad optical character recognition (OCR).\n",
        "Do not add any information that is not in the original text!\"\"\"\n",
        "\n",
        "df_small_bad_ocr['completion'] = df_small_bad_ocr.progress_apply(apply_completions,system_message=system_message, user_message=user_message, axis=1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 97,
      "metadata": {
        "id": "wjFbCSDYtNx-"
      },
      "outputs": [],
      "source": [
        "df_small_bad_ocr.iloc[4]['chunks']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "XLsWShc7tKGs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0141cb78-4cfb-4596-d08f-f568dfe5dcea"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Here is the transcribed text with corrections:\n",
            "\n",
            "###Discoursing upon things Italian, and although we have here no serious contribution to Venetian history, we have a volume packed full of valuable and interesting \"Gleanings\" presented in a most readable and easily digested form. Indeed, it may be questioned whether Mr. Crawford's 800 odd pages of story and legend will not be the means of conveying more general information to the average person concerning the formation, early being, and later development of the great Republic of the Adriatic than would a more ambitious work displaying all the fruits of wide study, careful research, and great erudition in the shape of carefully compiled statistics and precisely stated facts. Mr. Crawford has avoided everything of the kind, and hence to the general and more critical student of Venetian history and politics the book may be somewhat disappointing, not so, however, to the more casual reader, or tourist, to such, and their name is legion, it will prove full of interest and entertainment. The book, although not following any particular plan or arrangement, has nevertheless involved no small amount of labour; a fact that is indicated in the long list of writers consulted, which is given. At the end of the volume, together with a miniature\n"
          ]
        }
      ],
      "source": [
        "print(df_small_bad_ocr.iloc[4]['completion'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 98,
      "metadata": {
        "id": "6rc0U8WqsXiA"
      },
      "outputs": [],
      "source": [
        "df_small_bad_ocr.iloc[3]['chunks']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "Y5BxeDnttYZ7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 226
        },
        "outputId": "1e1a7769-be8b-44ba-bbce-f27df02f74b8"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Here is the transcribed text with corrections:\\n\\n### A portion of the chain taken with it £20 more. The Italian who gave immediate information at the II division station-house in Leman Street, Whitehall, had been waiting for time, and said the law for a few minutes, bemused in all the capitals of Europe, and had not been robbed before. THE \"Improvement\" in Hyde Park in the way of artificial do not seem to have given satisfaction generally. A deputation, representing the inhabitants of Park-side, Knightsbridge, has waited, by appointment, on the Right Hon. A. H. Layard, at the Office of Works, to draw his attention to the embankment and plantation which have been recently formed in Hyde Park, in the immediate rear of their houses. It was explained that the mound, together with the trees and shrubs, were a very great annoyance to the occupants, as they abut out light and air from the back rooms, and quite block the view of the park, hitherto enjoyed, besides rendering the houses damp and unhealthy. Mr. C. Mercier pointed out that the growth of the saplings which had been planted was very rapid, and in the course of a short time would quite block out the'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 51
        }
      ],
      "source": [
        "df_small_bad_ocr.iloc[3]['completion']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "BpChnh_7q-gF"
      },
      "outputs": [],
      "source": [
        "df_small_bad_ocr.to_csv('newspaper_ocr_corrected.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FzavYR10sI_o"
      },
      "source": [
        "## Exercise\n",
        "\n",
        "Experiment with your own system and user message! Have fun :-)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "OyrtAOQ24cJO"
      },
      "outputs": [],
      "source": [
        "# enter code here"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# What if things don't work?\n",
        "\n",
        "- Use larger models (see example below for using the OpenAI API)\n",
        "- Model fine-tuning on real or synthetic data. An example [here](https://huggingface.co/blog/mlabonne/sft-llama3)"
      ],
      "metadata": {
        "id": "21GtJijJbwlE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install openai"
      ],
      "metadata": {
        "id": "yzr930-ZeNtT"
      },
      "execution_count": 99,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import openai"
      ],
      "metadata": {
        "id": "SY4lisOheLoL"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_small_bad_ocr.iloc[4]['chunks']"
      ],
      "metadata": {
        "id": "UnMGcBDvdHiV"
      },
      "execution_count": 100,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from openai import OpenAI\n",
        "client = OpenAI(api_key='sk-...')"
      ],
      "metadata": {
        "id": "eChdTy-fsOiC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "completion = client.chat.completions.create(\n",
        "  model=\"gpt-4o\",\n",
        "  messages=[\n",
        "    {\"role\": \"system\", \"content\": \"You are a helpful assistant. Correct the text below.\"},\n",
        "    {\"role\": \"user\", \"content\": df_small_bad_ocr.iloc[4]['chunks']}\n",
        "  ]\n",
        ")\n",
        "\n"
      ],
      "metadata": {
        "id": "bz78uSV0bwRe"
      },
      "execution_count": 101,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(completion.choices[0].message.content)"
      ],
      "metadata": {
        "id": "-f-JzBjysGnP"
      },
      "execution_count": 102,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XzdNxsZPsKr5"
      },
      "source": [
        "# Fin."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "Tq0pWVJXoo10"
      ],
      "machine_shape": "hm",
      "provenance": [],
      "toc_visible": true,
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "9a2f073bffe24c87abb9d81547b43311": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_b2fb19d6b55c42a893b698ce22c02544",
              "IPY_MODEL_ff43f1349df542bc9dbe21a98d32a809",
              "IPY_MODEL_2e5dbb5d6c5d45afba265047753b9e20"
            ],
            "layout": "IPY_MODEL_0e050d898f1643ca8a5c074a554286b4"
          }
        },
        "b2fb19d6b55c42a893b698ce22c02544": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_56faa1ef2ea6445a8fdf1614e75e1c08",
            "placeholder": "​",
            "style": "IPY_MODEL_1451f4284e85462390c14e6057014c95",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "ff43f1349df542bc9dbe21a98d32a809": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6301aad1e0c046cba777e8eba409d55a",
            "max": 4,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_28b8de963d02438ca4a9ce3ca7ae7f04",
            "value": 4
          }
        },
        "2e5dbb5d6c5d45afba265047753b9e20": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_03284201464b44d7b5d200035c3404c2",
            "placeholder": "​",
            "style": "IPY_MODEL_4010561ef6e94f0caf25aa8166a49b06",
            "value": " 4/4 [00:11&lt;00:00,  2.38s/it]"
          }
        },
        "0e050d898f1643ca8a5c074a554286b4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "56faa1ef2ea6445a8fdf1614e75e1c08": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1451f4284e85462390c14e6057014c95": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6301aad1e0c046cba777e8eba409d55a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "28b8de963d02438ca4a9ce3ca7ae7f04": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "03284201464b44d7b5d200035c3404c2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4010561ef6e94f0caf25aa8166a49b06": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}